---
title: "Untitled"
author: "Nguyen_LSCM"
date: "3/28/2020"
output: html_document
---

#Set up libraries and datasets

```{r setup, include=FALSE}
library(caret)
library(AppliedPredictiveModeling)

str(iris)

```

#Visualization

##Scatterplot Matrix with pairs

```{r setup, include=FALSE}

transparentTheme(trans = .4)

featurePlot(x=iris[,1:4],y=iris$Species,aauto.key=list(columns=3))


```

##Scatterplot Matrix  with points

```{ r setup, include=FALSE}

pal<- c("red","blue","yellow")
featurePlot(x=iris[,1:4],y=iris$Species,plot="pairs",auto.key=list(columns=3,color=pal))



```

##Scatterplot Matrix with Ellipses

```{r setup, include=FALSE}

featurePlot(x=iris[,1:4],y=iris$Species,plot="ellipse",auto.key=list(columns=3))


```

##Overlayed Density Plot

```{r setup, include=FALSE}

transparentTheme(trans=0)
featurePlot(x=iris[,1:4],y=iris$Species,plot="density",scales=list(x=list(relation="free"),y=list(relation="free"),adjust=1.5,pch="|",layout=c(2,2),auto.key=list(columns=3)))


```

##Box Plots

```{r setup, include=FALSE}

featurePlot(x=iris[,1:4],y=iris$Species,scales=list(x=list(relation="free"),y=list(rot=45)),layout=c(4,1),auto.key=list(columns=3))

```


#Pre-Processing

<foot size=="5"> The data are numeric (factors have been converted to dummyvariables via ``model.matrix``, ``dummyVars`` or other means) </foot>

##Creating Dummy Variables

The function ``dummyVars`` can be used to generate a complete set of dummy variables from one or more factors

For example, the ``etitanic`` data set in the ``earth`` package includes 2 factors: ``pclass`` (passenger class, with levels 1st, 2nd, 3rd) and ``sex`` (with levels female, male). The base R function ``model.matrix`` would generate the following variables:

```{r setup, include=FALSE}

library(earth)
data(etitanic)
head(model.matrix(survived~., data=etitanic))

"Using dummyVars"

dummies <- dummyVars(survived~., data= etitanic)
head(predict(dummies,newdata = etitanic))



```
Note there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as ``lm``

## Zero - and Near Zero-Variance Predictors

In some situations, the data generating mechanism can create predictors that only have a single unique value ( a "zero- variance predictor"). For many models (excluding tree based models), this may cause the model to crash or the fit to be unstable.

Similarly, predictors might hve only  a handful of unique values that occur with very low frequencies


```{r, include=FALSE}
data(mdrr)
data.frame(table(mdrrDescr$nR11))

```

The concern here that these predictors may become zero-varable predictors when the data are split into cross-validation/boostrap sub-samples

To identify these typesof predictors, the following 2 metrics can be calculated:

- The frequency ratio
- The percent of unique values

If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might cnsider a predictor to be near-variance

Looking at the MDRR data, the ``nearZeroVar`` function can be used to identify near zero-variance variables (the ``saveMetrics`` argument can be used to show the details and usually defaults to FALSE):

```{r include=FALSE}

nzv<- nearZeroVar(mdrrDescr,saveMetrics = TRUE)

nzv[nzv$nzv,][1:10,]

nzv<- nearZeroVar(mdrrDescr)
"Share the column that have near zero Variance"

filteredDescr<- mdrrDescr[,-nzv]
dim(filteredDescr)

```

## Identifying Correlated Predictors

While there are some models that thrive on correlated predictors (such as ``pls``) other models may benefit from reducing the level of correlation between the predictors

Given a correlation matrix, the ``findCorrelation`` function uses the following algorithm to flag predictors for removal:

The matrix is replicate among upper.tri and lower.tri

```{r, include=FALSE}

descrCor<- cor(filteredDescr)
highCorr<- sum (abs(descrCor[upper.tri(descrCor)])>.999)

```

Before removing the high correlation 

```{r include=FALSE}

descrCor<-cor(filteredDescr)

summary(descrCor[upper.tri(descrCor)])

```

After removing the high correlation

```{r include=FALSE}

highlyCorDescr<- findCorrelation(descrCor,cutoff = .75)
filteredDescr<- filteredDescr[,-highlyCorDescr]

descrCor2<-cor(filteredDescr)
summary(descrCor2[upper.tri(descrCor2)])

```

## Linear Dependencies

The function ``findlinearCombos`` uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist)


```{r, include=FALSE}

ltfrDesign <- matrix(0, nrow=6, ncol=6)
ltfrDesign[,1] <- c(1, 1, 1, 1, 1, 1)
ltfrDesign[,2] <- c(1, 1, 1, 0, 0, 0)
ltfrDesign[,3] <- c(0, 0, 0, 1, 1, 1)
ltfrDesign[,4] <- c(1, 0, 0, 1, 0, 0)
ltfrDesign[,5] <- c(0, 1, 0, 0, 1, 0)
ltfrDesign[,6] <- c(0, 0, 1, 0, 0, 1)

comboInfo <- findLinearCombos(ltfrDesign)
comboInfo

ltfrDesign[, -comboInfo$remove]

```

##The ``preProcess`` Function

The ``preProcess`` class can be used for many operations on predictors, including **centering and scaling**

The function ``preProcess`` estimates the required parameters for each operation

``predict.preProcess`` is used to apply them to specific data sets

##Centering and Scalling

In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. ``preProcess`` doesn't actually pre-process the data. ``predict.preProcess`` is used to pre-process this and other data sets

```{r include=FALSE}

set.seed(96)

inTrain<- sample(seq(along=mdrrClass),length(mdrrClass)/2)

training<- filteredDescr[inTrain,]
test<- filteredDescr[-inTrain,]

preProcValues<-preProcess(training,method=c("center","scale"))

trainTransformed<- predict(preProcValues,training)
testTransformed<-predict(preProcValues,test)

```

## Imputation

- ``preProcess`` can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predicto is imputed using these values (using the mean)

## Transforming Predictors

In some cases there is a need to use principla component analysis (PCA) to transform the data to a smaller sub-space where the new variable are uncorredlate with 1 another. The ``preProcess`` class can apply this transformation by including "pca" in the "method" agreement. Going this will also force scaling of the predictors. Note that when PCA is requested, ``predict.preProcess`` changes the column names to ``PC1``, ``PC2`` and so on 

Similarly, independent component analysis (ICA) can also be used to find new variables that are linear combinations of the original set such that the components are independent (as opposed to uncorredlate in PCA). The new variables will be labeled as IC1, IC2 and so on
 
```{r include=FALSE}
 
 library(AppliedPredictiveModeling)
 transparentTheme(trans=.4)
 
plotSubset<- data.frame(scale(mdrrDescr[,c("nC","X4v")]))

xyplot(nC~X4v,data=plotSubset,groups=mdrrClass,auto.key=list(columns=2))

```
 
 After the spatial sign:
```{r include=FALSE}
 transformed<- spatialSign(plotSubset)
 transformed<- as.data.frame(transformed)
 

xyplot(nC~X4v,data-transformed, groups=mdrrClass,auto.key=list(columns=2))

```
Another option, "BoxCox" will estimate a Box-Cox tranformation on the predictors if the data are greater than zero

```{r include=FALSE}

preProcValues2<- preProcess(training,method="BoxCox")

trainBC<- predict(preProcValues2,training)
testBC<- predict(preProcValues2,test)
preProcValues2
 

```
 
 ## Putting it All Together
 
```{r include=FALSE}
 library(AppliedPredictiveModeling)
data("schedulingData")
str(schedulingData)

```
